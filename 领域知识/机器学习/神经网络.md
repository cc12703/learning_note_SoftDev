
# 神经网络

##  基础网络

### 结构

* 分为：输入层(一个)、隐藏层(多个)，输出层(一个)
* 层与层之间是全连接：`前一层的神经单元与下一层的所有神经单元都有连接`

![](https://gitee.com/cc12703/figurebed/raw/master/img/20201225194007.png)

#### 神经元

![](http://picbed.cc12703.com/20230624220448.png)





### 数学模型

![](https://gitee.com/cc12703/figurebed/raw/master/img/20201225201751.png)


| 符号 | 含义 |
| -- | -- | 
| $x_i$ | 输入层的第i个神经元的输入 |
| $a_j^l$ | 第l层的第j个神经元的输出 |
| $z_j^l$ | 第l层的第j个神经元的加权值 |
| $b_j^l$ | 第l层的第j个神经元的偏置 |
| $w_{ji}^l$ | 第l-1层的第i个神经元到第l层的第j个神经元的输入权重 |


* 加权公式：$ z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b $
* 输出公式：$ a = \sigma(z) $ **$\sigma$为激活函数**



### 自学习算法

* 目的：通过学习数据，确定网络中的权重参数和偏置参数
* 思路：计算出预测值和正解之间的误差（损失函数），确定参数让误差总和最小
* 方法：梯度下降法（直接求解太难）
    * 思路：将函数图像看成斜坡，沿最陡的方向一步一步的下降

#### 误差反向传播
* 目的：简化梯度的求解（直接求所有的偏导数太难）
* 思路：将导数计算替换成数列的递推
* 核心：将神经单元误差$\delta$作为中介，求出所有的偏导数


#### 公式

##### 前向传播
$$
a^l = \sigma(W^l a^{l-1} + b^l)
$$

说明：
* $W$为矩阵
* $b$, $a$为向量
* $\sigma$为激活函数


##### 损失函数
$$C_T = \sum_i C_i$$    
说明：
* 所有误差的总和

$$C = \frac{1}{2} (\sum_i(t_i - a_i)^2) $$
说明：
* 使用了均方误差


##### 反向传播
神经单位误差
$$
\delta_j^l = \frac{\partial C}{\partial z_j^l}
$$

参数偏导
$$
\frac{\partial C}{\partial w_{ji}^l} = \delta_j^l a_i^{l-1}
$$
$$
\frac{\partial C}{\partial b_j^l} = \delta_j^l
$$


单位误差递推公式
$$
\delta^l = (W^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)
$$


#### 流程
1. 使用随机数来初始化权重和偏置